#version 450

/**
 * GLSL forward propagation compute shader for Vulkan, by Dams 
 * Damien Balima (c) CC-BY-NC-SA-4.0 2024
 */

// One workgroup thread per neuron, one local thread per workgroup
layout (local_size_x = 1) in;

// Enum mapping, do not change
const int ELU = 0;
const int LReLU = 1;
const int PReLU = 2;
const int ReLU = 3;
const int Sigmoid = 4;
const int Tanh = 5;

struct RGBA {
    vec4 value;
};

struct Neuron {
    RGBA value;
    uint weightsIndex;
};

// will contains previous layer neurons here
layout (std430, binding = 0) buffer InputBuffer {
    Neuron neurons[];
} inputBuffer;

// will contains current layer neurons updated values
layout (std430, binding = 1) buffer OutputBuffer {
    vec4 values[];
} outputBuffer;

// will contains current layer neurons
layout (std430, binding = 2) buffer CurrentBuffer {
    Neuron neurons[];
} currentBuffer;

// will contains current layer activation function
layout (std430, binding = 3) buffer ActivationFunction {
    int function;
    float alpha;
} activationFunction;

// will contains current layer flatten neurons weights (indexed)
layout (std430, binding = 4) buffer Weights {
    RGBA values[];
} weights;

vec4 activateFunction(vec4 value){
   switch (activationFunction.function) {
        case ELU:        
            bvec4 mask = greaterThanEqual(value, vec4(0.0));
            value = mix(vec4(activationFunction.alpha) * (exp(value) - vec4(1.0)), value, mask);
            value = clamp(value, vec4(0.0), vec4(1.0));
        break;
        case LReLU: 
            value = clamp(0.01*value, vec4(0.0), vec4(1.0));
        break;
        case PReLU:
            value = max(activationFunction.alpha * value, value);
            value = clamp(value, vec4(0.0), vec4(1.0));
        break;
        case ReLU: 
            value = clamp(value, vec4(0.0), vec4(1.0));            
        break;
        case Sigmoid:
            value = 1.0 / (1.0 + exp(-value));
            value = clamp(value, vec4(0.0), vec4(1.0));
        break;
        case Tanh: // shifted tanh [-1,1] to [0,1]
            value = (tanh(value)/2.0) + 0.5;
        break;
        default:
            value = clamp(value, vec4(0.0), vec4(1.0));            
        break;
    }
    return value;
}

void main() {
    // Get the index of the neuron (one workgroup thread per neuron)
    uint index = gl_GlobalInvocationID.x;    
    vec4 value = vec4(0.0);
    // Get the index of the neuron weights in the flatten all neurons weights vector of current layer
    uint weightsIndex = currentBuffer.neurons[index].weightsIndex;

    // Calculate forward propagation using previous layer (inputBuffer) and weights of those connections with the neuron
    for (uint i = 0; i < inputBuffer.neurons.length(); i++) {
        value += (inputBuffer.neurons[i].value.value * weights.values[weightsIndex + i].value);
    }
    value = activateFunction(value); 

    // Put the results in the outputBuffer
    outputBuffer.values[index] = value;    
}
