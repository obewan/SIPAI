#version 450

// enable debug printf extension only in debug mode
// for debug printf infos, see https://www.lunarg.com/wp-content/uploads/2021/08/Using-Debug-Printf-02August2021.pdf
//#extension GL_EXT_debug_printf : enable

/**
 * *TEMPLATE* GLSL enhancer compute shader for Vulkan, by Dams
 * Damien Balima (c) CC-BY-NC-SA-4.0 2024
 * Do not edit
 */

// One workgroup, neurons local threads per workgroup
layout(local_size_x = %%MAX_SIZE_X%%, local_size_y = %%MAX_SIZE_Y%%) in;

// Enum mapping, do not change
const int ELU = 0;
const int LReLU = 1;
const int PReLU = 2;
const int ReLU = 3;
const int Sigmoid = 4;
const int Tanh = 5;

struct Neighbor {
  uint is_used;
  uint index_x;
  uint index_y;
  vec4 weight; // weight of the neighbor connection
};

struct HiddenNeuron {
  uint index_x;
  uint index_y;
  vec4 weights[%%INPUT_SIZE_Y%%][%%INPUT_SIZE_X%%];
  Neighbor neighbors[4];
};

struct OutputNeuron {
  uint index_x;
  uint index_y;
  vec4 weights[%%HIDDEN_SIZE_Y%%][%%HIDDEN_SIZE_X%%];
  Neighbor neighbors[4];
};

// Parameters binding
layout(std430, binding = 0) buffer readonly Parameters {
  float learning_rate;
  float error_min;
  float error_max;
}
params;

// Layers binding
layout(std430, binding = 1) buffer readonly InputLayer {
  float activation_alpha;
  uint activation_function;
  uint size_x;
  uint size_y;
}
inputLayerBuffer;

layout(std430, binding = 2) buffer OutputLayer {
  OutputNeuron neurons[%%OUTPUT_SIZE_Y%%][%%OUTPUT_SIZE_X%%];
  vec4 errors[%%OUTPUT_SIZE_Y%%][%%OUTPUT_SIZE_X%%];
  float activation_alpha;
  uint activation_function;
  uint size_x;
  uint size_y;
}
outputLayerBuffer;

layout(std430, binding = 3) buffer HiddenLayer1 {
  HiddenNeuron neurons[%%HIDDEN_SIZE_Y%%][%%HIDDEN_SIZE_X%%];
  vec4 values[%%HIDDEN_SIZE_Y%%][%%HIDDEN_SIZE_X%%];
  vec4 errors[%%HIDDEN_SIZE_Y%%][%%HIDDEN_SIZE_X%%];
  float activation_alpha;
  uint activation_function;
  uint size_x;
  uint size_y;
}
hiddenLayer1Buffer;

// Data binding
layout(std430, binding = 4) buffer readonly InputData {
  vec4 inputValues[%%INPUT_SIZE_Y%%][%%INPUT_SIZE_X%%];  
  vec4 targetValues[%%OUTPUT_SIZE_Y%%][%%OUTPUT_SIZE_X%%];  
  bool is_validation;
}
inputDataBuffer;

layout(std430, binding = 5) buffer writeonly OutputData { 
  vec4 outputValues[%%OUTPUT_SIZE_Y%%][%%OUTPUT_SIZE_X%%];
}
outputDataBuffer;

layout(std430, binding = 6) buffer writeonly OutputLoss { 
  float loss;
}
lossBuffer;

// Functions
vec4 derivativeFunction(vec4 value, uint activation_function,
                        float activation_alpha) {
  bvec4 mask;
  switch (activation_function) {
  case ELU:
    mask = greaterThanEqual(value, vec4(0.0));
    value = mix(vec4(1.0), activation_alpha * exp(value), mask);
    break;
  case LReLU:
    value = clamp(value, 0.01, 1.0);
    break;
  case PReLU:
    mask = greaterThanEqual(value, vec4(0.0));
    value = mix(vec4(1.0), vec4(activation_alpha), mask);
    break;
  case ReLU:
    mask = greaterThanEqual(value, vec4(0.0));
    value = mix(vec4(1.0), vec4(0.0), mask);
    break;
  case Sigmoid:
    value *= (1 - value);
    break;
  case Tanh: // shifted tanh [-1,1] to [0,1]
    value = (tanh(value) / 2.0) + 0.5;
    value = 1 - value * value;
    break;
  default:
    break;
  }
  return value;
}

vec4 activateFunction(vec4 value, uint activation_function,
                      float activation_alpha) {
  bvec4 mask;
  switch (activation_function) {
  case ELU:
    mask = greaterThanEqual(value, vec4(0.0));
    value = mix(vec4(activation_alpha) * (exp(value) - vec4(1.0)), value, mask);
    value = clamp(value, 0.0, 1.0);
    break;
  case LReLU:
    value = clamp(0.01 * value, 0.0, 1.0);
    break;
  case PReLU:
    value = max(activation_alpha * value, value);
    value = clamp(value, 0.0, 1.0);
    break;
  case ReLU:
    value = clamp(value, 0.0, 1.0);
    break;
  case Sigmoid:
    value = 1.0 / (1.0 + exp(-value));
    value = clamp(value, 0.0, 1.0);
    break;
  case Tanh: // shifted tanh [-1,1] to [0,1]
    value = (tanh(value) / 2.0) + 0.5;
    break;
  default:
    break;
  }
  return value;
}

void forwardHiddenLayer1(uint index_x, uint index_y) {
  if (index_y >= %%HIDDEN_SIZE_Y%% || index_x >= %%HIDDEN_SIZE_X%%) {
    return;
  }
  // forward hidden layer using previous input layer 
  vec4 result = vec4(0.0);
  for (uint y = 0; y < %%INPUT_SIZE_Y%%; y++) {
    for (uint x = 0; x < %%INPUT_SIZE_X%%; x++) {
      result += inputDataBuffer.inputValues[y][x] *
                hiddenLayer1Buffer.neurons[index_y][index_x].weights[y][x];
    }
  }
  hiddenLayer1Buffer.values[index_y][index_x] = activateFunction(
      result, hiddenLayer1Buffer.activation_function, hiddenLayer1Buffer.activation_alpha);
  //debugPrintfEXT("[DEBUG][FORWARDHIDDENLAYER1] hiddenLayer1Buffer.values[%i][%i] = %v4f", index_y, index_x, hiddenLayer1Buffer.values[index_y][index_x]);
}

void forwardOutputLayer(uint index_x, uint index_y) {
  if (index_y >= %%OUTPUT_SIZE_Y%% || index_x >= %%OUTPUT_SIZE_X%%) {
    return;
  }
  // forward output layer using previous hidden layer
  vec4 result = vec4(0.0);
  for (uint y = 0; y < %%HIDDEN_SIZE_Y%%; y++) {
    for (uint x = 0; x < %%HIDDEN_SIZE_X%%; x++) {
      result += hiddenLayer1Buffer.values[y][x] *
                outputLayerBuffer.neurons[index_y][index_x].weights[y][x];
    }
  }
  outputDataBuffer.outputValues[index_y][index_x] = activateFunction(
      result, outputLayerBuffer.activation_function, outputLayerBuffer.activation_alpha);
  //debugPrintfEXT("[DEBUG][FORWARDOUTPUTLAYER] outputDataBuffer.outputValues[%i][%i] = %v4f", index_y, index_x, outputDataBuffer.outputValues[index_y][index_x]);
}


void main() {
  // Get the index of the neuron (one local thread per neuron)
  uint index_x = gl_LocalInvocationID.x;
  uint index_y = gl_LocalInvocationID.y;

  // Forward Propagation
  forwardHiddenLayer1(index_x, index_y);
  barrier();
  forwardOutputLayer(index_x, index_y);
  barrier();
}