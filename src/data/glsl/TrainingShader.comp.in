#version 450
#extension GL_EXT_shader_atomic_float : enable

/**
 * *TEMPLATE* GLSL training compute shader for Vulkan, by Dams
 * Damien Balima (c) CC-BY-NC-SA-4.0 2024
 * Do not edit
 */

// One workgroup, neurons local threads per workgroup
layout(local_size_x = %%MAX_SIZE_X%%, local_size_y = %%MAX_SIZE_Y%%) in;

// Enum mapping, do not change
const int ELU = 0;
const int LReLU = 1;
const int PReLU = 2;
const int ReLU = 3;
const int Sigmoid = 4;
const int Tanh = 5;

struct Neighbor {
  bool is_used;
  uint index_x;
  uint index_y;
  vec4 weight; // weight of the neighbor connection
};

struct HiddenNeuron {
  uint index_x;
  uint index_y;
  vec4 weights[%%INPUT_SIZE_X%%][%%INPUT_SIZE_Y%%];
  Neighbor neighbors[4];
};

struct OutputNeuron {
  uint index_x;
  uint index_y;
  vec4 weights[%%HIDDEN_SIZE_X%%][%%HIDDEN_SIZE_Y%%];
  Neighbor neighbors[4];
};

// Parameters binding
layout(std430, binding = 0) buffer readonly Parameters {
  float learning_rate;
  float error_min;
  float error_max;
}
params;

// Data binding
layout(std430, binding = 1) buffer Data {
  vec4 inputValues[%%INPUT_SIZE_X%%][%%INPUT_SIZE_Y%%];
  vec4 outputValues[%%OUTPUT_SIZE_X%%][%%OUTPUT_SIZE_Y%%];
  vec4 targetValues[%%OUTPUT_SIZE_X%%][%%OUTPUT_SIZE_Y%%];
  float loss;
  bool is_validation;
}
data;

// Layers binding
layout(std430, binding = 2) buffer InputLayer {
  float activation_alpha;
  uint activation_function;
  uint size_x;
  uint size_y;
}
inputLayer;

layout(std430, binding = 3) buffer OutputLayer {
  OutputNeuron neurons[%%OUTPUT_SIZE_X%%][%%OUTPUT_SIZE_Y%%];
  vec4 errors[%%OUTPUT_SIZE_X%%][%%OUTPUT_SIZE_Y%%];
  float activation_alpha;
  uint activation_function;
  uint size_x;
  uint size_y;
}
outputLayer;

layout(std430, binding = 4) buffer HiddenLayer1 {
  HiddenNeuron neurons[%%HIDDEN_SIZE_X%%][%%HIDDEN_SIZE_Y%%];
  vec4 values[%%HIDDEN_SIZE_X%%][%%HIDDEN_SIZE_Y%%];
  vec4 errors[%%HIDDEN_SIZE_X%%][%%HIDDEN_SIZE_Y%%];
  float activation_alpha;
  uint activation_function;
  uint size_x;
  uint size_y;
}
hiddenLayer1;

// Functions
vec4 derivativeFunction(vec4 value, uint activation_function,
                        float activation_alpha) {
  bvec4 mask;
  switch (activation_function) {
  case ELU:
    mask = greaterThanEqual(value, vec4(0.0));
    value = mix(vec4(1.0), activation_alpha * exp(value), mask);
    break;
  case LReLU:
    value = clamp(value, 0.01, 1.0);
    break;
  case PReLU:
    mask = greaterThanEqual(value, vec4(0.0));
    value = mix(vec4(1.0), vec4(activation_alpha), mask);
    break;
  case ReLU:
    mask = greaterThanEqual(value, vec4(0.0));
    value = mix(vec4(1.0), vec4(0.0), mask);
    break;
  case Sigmoid:
    value *= (1 - value);
    break;
  case Tanh: // shifted tanh [-1,1] to [0,1]
    value = (tanh(value) / 2.0) + 0.5;
    value = 1 - value * value;
    break;
  default:
    break;
  }
  return value;
}

vec4 activateFunction(vec4 value, uint activation_function,
                      float activation_alpha) {
  bvec4 mask;
  switch (activation_function) {
  case ELU:
    mask = greaterThanEqual(value, vec4(0.0));
    value = mix(vec4(activation_alpha) * (exp(value) - vec4(1.0)), value, mask);
    value = clamp(value, 0.0, 1.0);
    break;
  case LReLU:
    value = clamp(0.01 * value, 0.0, 1.0);
    break;
  case PReLU:
    value = max(activation_alpha * value, value);
    value = clamp(value, 0.0, 1.0);
    break;
  case ReLU:
    value = clamp(value, 0.0, 1.0);
    break;
  case Sigmoid:
    value = 1.0 / (1.0 + exp(-value));
    value = clamp(value, 0.0, 1.0);
    break;
  case Tanh: // shifted tanh [-1,1] to [0,1]
    value = (tanh(value) / 2.0) + 0.5;
    break;
  default:
    break;
  }
  return value;
}

void forwardPropagationHiddenLayer1(uint index_x, uint index_y) {
  if (index_x >= hiddenLayer1.values.length() ||
      index_y >= hiddenLayer1.values[0].length()) {
    return;
  }
  // forward hidden layer
  vec4 result = vec4(0.0);
  for (uint y = 0; y < data.inputValues.length(); y++) {
    for (uint x = 0; x < data.inputValues[0].length(); x++) {
      result += data.inputValues[x][y] *
                hiddenLayer1.neurons[index_y][index_x].weights[x][y];
    }
  }
  hiddenLayer1.values[index_x][index_y] = activateFunction(
      result, hiddenLayer1.activation_function, hiddenLayer1.activation_alpha);
}

void forwardPropagationOutputLayer(uint index_x, uint index_y) {
  if (index_x >= data.outputValues.length() ||
      index_y >= data.outputValues[0].length()) {
    return;
  }
  // forward output layer
  vec4 result = vec4(0.0);
  for (uint y = 0; y < hiddenLayer1.values.length(); y++) {
    for (uint x = 0; x < hiddenLayer1.values[0].length(); x++) {
      result += hiddenLayer1.values[x][y] *
                outputLayer.neurons[index_y][index_x].weights[x][y];
    }
  }
  data.outputValues[index_x][index_y] = activateFunction(
      result, outputLayer.activation_function, outputLayer.activation_alpha);
}

// Using mean square error (MSE)
void computeLoss(uint index_x, uint index_y) {
  if (index_x >= data.outputValues.length() ||
      index_y >= data.outputValues[0].length()) {
    return;
  }
  vec4 diff =
      data.outputValues[index_x][index_y] - data.targetValues[index_x][index_y];
  float squaredDiff = dot(diff, diff);
  atomicAdd(data.loss, squaredDiff);
}

void computeOutputError(uint index_x, uint index_y) {
  if (index_y >= outputLayer.neurons.length() ||
      index_x >= outputLayer.neurons[0].length()) {
    return;
  }
  // add neighbors connections weighted values
  vec4 neighborSum = vec4(0.0);
  for (uint i = 0; i < outputLayer.neurons[index_y][index_x].neighbors.length();
       i++) {
    uint neighbor_x =
        outputLayer.neurons[index_y][index_x].neighbors[i].index_x;
    uint neighbor_y =
        outputLayer.neurons[index_y][index_x].neighbors[i].index_y;
    neighborSum += outputLayer.neurons[index_y][index_x].neighbors[i].weight *
                   data.outputValues[neighbor_x][neighbor_y];
  }

  // Compute and update the error
  float weightFactor = 0.5; // Experiment with weight between 0 and 1
  vec4 outputValue = data.outputValues[index_x][index_y];
  vec4 targetValue = data.targetValues[index_x][index_y];
  vec4 newError = weightFactor * (outputValue - targetValue) +
                  ((1.0 - weightFactor) * neighborSum);
  outputLayer.errors[index_x][index_y] =
      clamp(newError, params.error_min, params.error_max);
}

void backwardHiddenLayer1(uint index_x, uint index_y) {
  if (index_y >= hiddenLayer1.neurons.length() ||
      index_x >= hiddenLayer1.neurons[0].length()) {
    return;
  }
  vec4 error = vec4(0.0);

  // Add next layer neurons error ponderated with weights for this neuron
  for (uint out_y = 0; out_y < outputLayer.neurons.length(); out_y++) {
    for (uint out_x = 0; out_x < outputLayer.neurons[0].length(); out_x++) {
      vec4 out_error = outputLayer.errors[out_x][out_y];
      vec4 out_weight =
          outputLayer.neurons[out_y][out_x].weights[index_x][index_y];
      error += (out_error * out_weight);
    }
  }

  // Consider errors of adjacent neurons
  for (uint i = 0;
       i < hiddenLayer1.neurons[index_y][index_x].neighbors.length(); i++) {
    if (!hiddenLayer1.neurons[index_y][index_x].neighbors[i].is_used) {
      continue;
    }
    error += hiddenLayer1.neurons[index_y][index_x].neighbors[i].weight *
             hiddenLayer1.errors
                 [hiddenLayer1.neurons[index_y][index_x].neighbors[i].index_x]
                 [hiddenLayer1.neurons[index_y][index_x].neighbors[i].index_y];
  }

  // Use the derivative of the activation function
  vec4 derivatedError = derivativeFunction(
      error, hiddenLayer1.activation_function, hiddenLayer1.activation_alpha);
  hiddenLayer1.errors[index_x][index_y] =
      clamp(derivatedError, params.error_min, params.error_max);
}

void updateWeightsHiddenLayer1(uint index_x, uint index_y) {
  if (index_y >= hiddenLayer1.neurons.length() ||
      index_x >= hiddenLayer1.neurons[0].length()) {
    return;
  }
  vec4 learningRateError =
      hiddenLayer1.errors[index_x][index_y] * params.learning_rate;

  // Update neuron weights that are connections weights with previous layers
  for (uint x = 0; x < hiddenLayer1.neurons[index_y][index_x].weights.length();
       x++) {
    for (uint y = 0;
         y < hiddenLayer1.neurons[index_y][index_x].weights[0].length(); y++) {
      hiddenLayer1.neurons[index_y][index_x].weights[x][y] -=
          (data.inputValues[x][y] * learningRateError);
    }
  }

  // Update neighbors connections weights
  for (uint i = 0;
       i < hiddenLayer1.neurons[index_y][index_x].neighbors.length(); i++) {
    if (!hiddenLayer1.neurons[index_y][index_x].neighbors[i].is_used) {
      continue;
    }
    hiddenLayer1.neurons[index_y][index_x].neighbors[i].weight -=
        hiddenLayer1.values
            [hiddenLayer1.neurons[index_y][index_x].neighbors[i].index_x]
            [hiddenLayer1.neurons[index_y][index_x].neighbors[i].index_y] *
        learningRateError;
  }
}

void updateWeightsOutputLayer(uint index_x, uint index_y) {
  if (index_y >= outputLayer.neurons.length() ||
      index_x >= outputLayer.neurons[0].length()) {
    return;
  }
  vec4 learningRateError =
      outputLayer.errors[index_x][index_y] * params.learning_rate;

  // Update neuron weights that are connections weights with previous layers
  for (uint x = 0; x < outputLayer.neurons[index_y][index_x].weights.length();
       x++) {
    for (uint y = 0;
         y < outputLayer.neurons[index_y][index_x].weights[0].length(); y++) {
      outputLayer.neurons[index_y][index_x].weights[x][y] -=
          (hiddenLayer1.values[x][y] * learningRateError);
    }
  }

  // Update neighbors connections weights
  for (uint i = 0; i < outputLayer.neurons[index_y][index_x].neighbors.length();
       i++) {
    if (!outputLayer.neurons[index_y][index_x].neighbors[i].is_used) {
      continue;
    }
    outputLayer.neurons[index_y][index_x].neighbors[i].weight -=
        data.outputValues
            [outputLayer.neurons[index_y][index_x].neighbors[i].index_x]
            [outputLayer.neurons[index_y][index_x].neighbors[i].index_y] *
        learningRateError;
  }
}

void main() {
  // Get the index of the neuron (one local thread per neuron)
  uint index_x = gl_LocalInvocationID.x;
  uint index_y = gl_LocalInvocationID.y;

  // Forward Propagation
  forwardPropagationHiddenLayer1(index_x, index_y);
  barrier();
  forwardPropagationOutputLayer(index_x, index_y);
  barrier();

  // Comput Loss
  computeLoss(index_x, index_y);
  barrier();
  if (index_x == 0 && index_y == 0) {
    uint numPixels = data.targetValues.length() * data.targetValues[0].length();
    data.loss /= numPixels;
  }
  barrier();

  // Backward Propagation (not for validation)
  if (data.is_validation) {
    return;
  }
  computeOutputError(index_x, index_y);
  barrier();
  backwardHiddenLayer1(index_x, index_y);
  barrier();
  updateWeightsHiddenLayer1(index_x, index_y);
  barrier();
  updateWeightsOutputLayer(index_x, index_y);
  barrier();
}