#version 450

/**
 * *TEMPLATE* GLSL training compute shader for Vulkan, by Dams
 * Damien Balima (c) CC-BY-NC-SA-4.0 2024
 * Do not edit
 */

// One workgroup, neurons local threads per workgroup
layout(local_size_x = %%MAX_SIZE_X%%, local_size_y = %%MAX_SIZE_Y%%) in;

// thread shared datas (warning it is a fast but very limited memory)
shared vec4 sharedOutputValues[%%OUTPUT_SIZE_Y%%][%%OUTPUT_SIZE_X%%];
shared float sharedOutputLoss[%%OUTPUT_SIZE_Y%%][%%OUTPUT_SIZE_X%%];

// Enum mapping, do not change
const int ELU = 0;
const int LReLU = 1;
const int PReLU = 2;
const int ReLU = 3;
const int Sigmoid = 4;
const int Tanh = 5;

struct Neighbor {
  uint is_used;
  uint index_x;
  uint index_y;
  vec4 weight; // weight of the neighbor connection
};

struct HiddenNeuron {
  uint index_x;
  uint index_y;
  vec4 weights[%%INPUT_SIZE_Y%%][%%INPUT_SIZE_X%%];
  Neighbor neighbors[4];
};

struct OutputNeuron {
  uint index_x;
  uint index_y;
  vec4 weights[%%HIDDEN_SIZE_Y%%][%%HIDDEN_SIZE_X%%];
  Neighbor neighbors[4];
};

// Parameters binding
layout(std430, binding = 0) buffer readonly Parameters {
  float learning_rate;
  float error_min;
  float error_max;
}
params;

// Layers binding
layout(std430, binding = 1) buffer InputLayer {
  float activation_alpha;
  uint activation_function;
  uint size_x;
  uint size_y;
}
inputLayerBuffer;

layout(std430, binding = 2) buffer OutputLayer {
  OutputNeuron neurons[%%OUTPUT_SIZE_Y%%][%%OUTPUT_SIZE_X%%];
  vec4 errors[%%OUTPUT_SIZE_Y%%][%%OUTPUT_SIZE_X%%];
  float activation_alpha;
  uint activation_function;
  uint size_x;
  uint size_y;
}
outputLayerBuffer;

layout(std430, binding = 3) buffer HiddenLayer1 {
  HiddenNeuron neurons[%%HIDDEN_SIZE_Y%%][%%HIDDEN_SIZE_X%%];
  vec4 values[%%HIDDEN_SIZE_Y%%][%%HIDDEN_SIZE_X%%];
  vec4 errors[%%HIDDEN_SIZE_Y%%][%%HIDDEN_SIZE_X%%];
  float activation_alpha;
  uint activation_function;
  uint size_x;
  uint size_y;
}
hiddenLayer1Buffer;

// Data binding
layout(std430, binding = 4) buffer readonly InputData {
  vec4 inputValues[%%INPUT_SIZE_Y%%][%%INPUT_SIZE_X%%];  
  vec4 targetValues[%%OUTPUT_SIZE_Y%%][%%OUTPUT_SIZE_X%%];  
  bool is_validation;
}
inputDataBuffer;

layout(std430, binding = 5) buffer writeonly OutputData { 
  vec4 outputValues[%%OUTPUT_SIZE_XY%%];
}
outputDataBuffer;

layout(std430, binding = 6) buffer writeonly OutputLoss { 
  float loss;
}
lossBuffer;

// Functions
vec4 derivativeFunction(vec4 value, uint activation_function,
                        float activation_alpha) {
  bvec4 mask;
  switch (activation_function) {
  case ELU:
    mask = greaterThanEqual(value, vec4(0.0));
    value = mix(vec4(1.0), activation_alpha * exp(value), mask);
    break;
  case LReLU:
    value = clamp(value, 0.01, 1.0);
    break;
  case PReLU:
    mask = greaterThanEqual(value, vec4(0.0));
    value = mix(vec4(1.0), vec4(activation_alpha), mask);
    break;
  case ReLU:
    mask = greaterThanEqual(value, vec4(0.0));
    value = mix(vec4(1.0), vec4(0.0), mask);
    break;
  case Sigmoid:
    value *= (1 - value);
    break;
  case Tanh: // shifted tanh [-1,1] to [0,1]
    value = (tanh(value) / 2.0) + 0.5;
    value = 1 - value * value;
    break;
  default:
    break;
  }
  return value;
}

vec4 activateFunction(vec4 value, uint activation_function,
                      float activation_alpha) {
  bvec4 mask;
  switch (activation_function) {
  case ELU:
    mask = greaterThanEqual(value, vec4(0.0));
    value = mix(vec4(activation_alpha) * (exp(value) - vec4(1.0)), value, mask);
    value = clamp(value, 0.0, 1.0);
    break;
  case LReLU:
    value = clamp(0.01 * value, 0.0, 1.0);
    break;
  case PReLU:
    value = max(activation_alpha * value, value);
    value = clamp(value, 0.0, 1.0);
    break;
  case ReLU:
    value = clamp(value, 0.0, 1.0);
    break;
  case Sigmoid:
    value = 1.0 / (1.0 + exp(-value));
    value = clamp(value, 0.0, 1.0);
    break;
  case Tanh: // shifted tanh [-1,1] to [0,1]
    value = (tanh(value) / 2.0) + 0.5;
    break;
  default:
    break;
  }
  return value;
}

void forwardPropagationHiddenLayer1(uint index_x, uint index_y) {
  if (index_y >= hiddenLayer1Buffer.values.length() ||
      index_x >= hiddenLayer1Buffer.values[0].length()) {
    return;
  }
  // forward hidden layer
  vec4 result = vec4(0.0);
  for (uint y = 0; y < inputDataBuffer.inputValues.length(); y++) {
    for (uint x = 0; x < inputDataBuffer.inputValues[0].length(); x++) {
      result += inputDataBuffer.inputValues[y][x] *
                hiddenLayer1Buffer.neurons[index_y][index_x].weights[y][x];
    }
  }
  hiddenLayer1Buffer.values[index_y][index_x] = activateFunction(
      result, hiddenLayer1Buffer.activation_function, hiddenLayer1Buffer.activation_alpha);
}

void forwardPropagationOutputLayer(uint index_x, uint index_y) {
  if (index_y >= sharedOutputValues.length() ||
      index_x >= sharedOutputValues[0].length()) {
    return;
  }
  // forward output layer
  vec4 result = vec4(0.0);
  for (uint y = 0; y < hiddenLayer1Buffer.values.length(); y++) {
    for (uint x = 0; x < hiddenLayer1Buffer.values[0].length(); x++) {
      result += hiddenLayer1Buffer.values[y][x] *
                outputLayerBuffer.neurons[index_y][index_x].weights[y][x];
    }
  }
  sharedOutputValues[index_y][index_x] = activateFunction(
      result, outputLayerBuffer.activation_function, outputLayerBuffer.activation_alpha);
}

// Using mean square error (MSE)
void computeLoss(uint index_x, uint index_y) {
  if (index_y >= sharedOutputValues.length() ||
      index_x >= sharedOutputValues[0].length()) {
    return;
  }
  vec4 diff = sharedOutputValues[index_y][index_x] - 
              inputDataBuffer.targetValues[index_y][index_x];
  float squaredDiff = dot(diff, diff);
  sharedOutputLoss[index_y][index_x] = squaredDiff;
}

void computeOutputError(uint index_x, uint index_y) {
  if (index_y >= outputLayerBuffer.neurons.length() ||
      index_x >= outputLayerBuffer.neurons[0].length()) {
    return;
  }
  // add neighbors connections weighted values
  vec4 neighborSum = vec4(0.0);
  for (uint i = 0; i < outputLayerBuffer.neurons[index_y][index_x].neighbors.length();
       i++) {
    uint neighbor_x =
        outputLayerBuffer.neurons[index_y][index_x].neighbors[i].index_x;
    uint neighbor_y =
        outputLayerBuffer.neurons[index_y][index_x].neighbors[i].index_y;
    neighborSum += outputLayerBuffer.neurons[index_y][index_x].neighbors[i].weight *
                   sharedOutputValues[neighbor_y][neighbor_x];
  }

  // Compute and update the error
  float weightFactor = 0.5f; // Experiment with weight between 0 and 1
  vec4 outputValue = sharedOutputValues[index_y][index_x];
  vec4 targetValue = inputDataBuffer.targetValues[index_y][index_x];
  vec4 newError = weightFactor * (outputValue - targetValue) +
                  ((1.0 - weightFactor) * neighborSum);
  outputLayerBuffer.errors[index_y][index_x] =
      clamp(newError, params.error_min, params.error_max);
}

void backwardHiddenLayer1(uint index_x, uint index_y) {
  if (index_y >= hiddenLayer1Buffer.neurons.length() ||
      index_x >= hiddenLayer1Buffer.neurons[0].length()) {
    return;
  }
  vec4 error = vec4(0.0);

  // Add next layer neurons error ponderated with weights for this neuron
  for (uint out_y = 0; out_y < outputLayerBuffer.neurons.length(); out_y++) {
    for (uint out_x = 0; out_x < outputLayerBuffer.neurons[0].length(); out_x++) {
      vec4 out_error = outputLayerBuffer.errors[out_y][out_x];
      vec4 out_weight =
          outputLayerBuffer.neurons[out_y][out_x].weights[index_y][index_x];
      error += (out_error * out_weight);
    }
  }

  // Consider errors of adjacent neurons
  for (uint i = 0;
       i < hiddenLayer1Buffer.neurons[index_y][index_x].neighbors.length(); i++) {
    if (hiddenLayer1Buffer.neurons[index_y][index_x].neighbors[i].is_used == 0) {
      continue;
    }
    error += hiddenLayer1Buffer.neurons[index_y][index_x].neighbors[i].weight *
             hiddenLayer1Buffer.errors
                 [hiddenLayer1Buffer.neurons[index_y][index_x].neighbors[i].index_y]
                 [hiddenLayer1Buffer.neurons[index_y][index_x].neighbors[i].index_x];
  }

  // Use the derivative of the activation function
  vec4 derivatedError = derivativeFunction(
      error, hiddenLayer1Buffer.activation_function, hiddenLayer1Buffer.activation_alpha);
  hiddenLayer1Buffer.errors[index_y][index_x] =
      clamp(derivatedError, params.error_min, params.error_max);
}

void updateWeightsHiddenLayer1(uint index_x, uint index_y) {
  if (index_y >= hiddenLayer1Buffer.neurons.length() ||
      index_x >= hiddenLayer1Buffer.neurons[0].length()) {
    return;
  }
  vec4 learningRateError =
      hiddenLayer1Buffer.errors[index_y][index_x] * params.learning_rate;

  // Update neuron weights that are connections weights with previous layers
  for (uint y = 0; y < hiddenLayer1Buffer.neurons[index_y][index_x].weights.length();
       y++) {
    for (uint x = 0;
         x < hiddenLayer1Buffer.neurons[index_y][index_x].weights[0].length(); x++) {
      hiddenLayer1Buffer.neurons[index_y][index_x].weights[y][x] -=
          (inputDataBuffer.inputValues[y][x] * learningRateError);
    }
  }

  // Update neighbors connections weights
  for (uint i = 0;
       i < hiddenLayer1Buffer.neurons[index_y][index_x].neighbors.length(); i++) {
    if (hiddenLayer1Buffer.neurons[index_y][index_x].neighbors[i].is_used == 0) {
      continue;
    }
    hiddenLayer1Buffer.neurons[index_y][index_x].neighbors[i].weight -=
        hiddenLayer1Buffer.values
            [hiddenLayer1Buffer.neurons[index_y][index_x].neighbors[i].index_y]
            [hiddenLayer1Buffer.neurons[index_y][index_x].neighbors[i].index_x] *
        learningRateError;
  }
}

void updateWeightsOutputLayer(uint index_x, uint index_y) {
  if (index_y >= outputLayerBuffer.neurons.length() ||
      index_x >= outputLayerBuffer.neurons[0].length()) {
    return;
  }
  vec4 learningRateError =
      outputLayerBuffer.errors[index_y][index_x] * params.learning_rate;

  // Update neuron weights that are connections weights with previous layers
  for (uint y = 0; y < outputLayerBuffer.neurons[index_y][index_x].weights.length();
       y++) {
    for (uint x = 0;
         x < outputLayerBuffer.neurons[index_y][index_x].weights[0].length(); x++) {
      outputLayerBuffer.neurons[index_y][index_x].weights[y][x] -=
          (hiddenLayer1Buffer.values[y][x] * learningRateError);
    }
  }

  // Update neighbors connections weights
  for (uint i = 0; i < outputLayerBuffer.neurons[index_y][index_x].neighbors.length();
       i++) {
    if (outputLayerBuffer.neurons[index_y][index_x].neighbors[i].is_used == 0) {
      continue;
    }
    outputLayerBuffer.neurons[index_y][index_x].neighbors[i].weight -=
        sharedOutputValues
            [outputLayerBuffer.neurons[index_y][index_x].neighbors[i].index_y]
            [outputLayerBuffer.neurons[index_y][index_x].neighbors[i].index_x] *
        learningRateError;
  }
}

void main() {
  // Get the index of the neuron (one local thread per neuron)
  uint index_x = gl_LocalInvocationID.x;
  uint index_y = gl_LocalInvocationID.y;

  // Initialize variables
  if (index_y < %%OUTPUT_SIZE_Y%% && index_x < %%OUTPUT_SIZE_X%%) {
    sharedOutputValues[index_y][index_x] = vec4(0.0);
    sharedOutputLoss[index_y][index_x] = 0.0f;
  }
  barrier();

  // Forward Propagation
  forwardPropagationHiddenLayer1(index_x, index_y);
  barrier();
  forwardPropagationOutputLayer(index_x, index_y);
  barrier();

  // Comput Loss
  computeLoss(index_x, index_y);
  barrier();

  // Output writing (unique thread)
  if (gl_LocalInvocationIndex == 0) {   
    float totalLoss = 0.0f;
    for (int y = 0; y < %%OUTPUT_SIZE_Y%%; ++y) {
      for (int x = 0; x < %%OUTPUT_SIZE_X%%; ++x) {
        totalLoss += sharedOutputLoss[y][x];
      }
    }
    uint numPixels = %%OUTPUT_SIZE_Y%% * %%OUTPUT_SIZE_X%%;    
    if (numPixels == 0){
      numPixels = 1;
    }
    lossBuffer.loss = totalLoss / numPixels;

    // Flatten outputValues
    // Commented: not required for training
    // for (int y = 0; y <  %%OUTPUT_SIZE_Y%%; ++y) {
    //  for (int x = 0; x < %%OUTPUT_SIZE_X%%; ++x) {
    //    outputDataBuffer.outputValues[y * %%OUTPUT_SIZE_X%% + x] = sharedOutputValues[y][x];
    // }
    //}
  }
  barrier();

  // Backward Propagation (not for validation)
  if (inputDataBuffer.is_validation) {
    return;
  }
  computeOutputError(index_x, index_y);
  barrier();
  backwardHiddenLayer1(index_x, index_y);
  barrier();
  updateWeightsHiddenLayer1(index_x, index_y);
  barrier();
  updateWeightsOutputLayer(index_x, index_y);
}