<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>LCOV - lcov.info - src/NeuralNetwork.cpp</title>
  <link rel="stylesheet" type="text/css" href="../gcov.css">
</head>

<body>

  <table width="100%" border=0 cellspacing=0 cellpadding=0>
    <tr><td class="title">LCOV - code coverage report</td></tr>
    <tr><td class="ruler"><img src="../glass.png" width=3 height=3 alt=""></td></tr>

    <tr>
      <td width="100%">
        <table cellpadding=1 border=0 width="100%">
          <tr>
            <td width="10%" class="headerItem">Current view:</td>
            <td width="35%" class="headerValue"><a href="../index.html">top level</a> - <a href="index.html">src</a> - NeuralNetwork.cpp<span style="font-size: 80%;"> (source / <a href="NeuralNetwork.cpp.func-sort-c.html">functions</a>)</span></td>
            <td width="5%"></td>
            <td width="15%"></td>
            <td width="10%" class="headerCovTableHead">Hit</td>
            <td width="10%" class="headerCovTableHead">Total</td>
            <td width="15%" class="headerCovTableHead">Coverage</td>
          </tr>
          <tr>
            <td class="headerItem">Test:</td>
            <td class="headerValue">lcov.info</td>
            <td></td>
            <td class="headerItem">Lines:</td>
            <td class="headerCovTableEntry">94</td>
            <td class="headerCovTableEntry">121</td>
            <td class="headerCovTableEntryMed">77.7 %</td>
          </tr>
          <tr>
            <td class="headerItem">Date:</td>
            <td class="headerValue">2024-03-20 22:38:05</td>
            <td></td>
            <td class="headerItem">Functions:</td>
            <td class="headerCovTableEntry">9</td>
            <td class="headerCovTableEntry">13</td>
            <td class="headerCovTableEntryLo">69.2 %</td>
          </tr>
          <tr><td><img src="../glass.png" width=3 height=3 alt=""></td></tr>
        </table>
      </td>
    </tr>

    <tr><td class="ruler"><img src="../glass.png" width=3 height=3 alt=""></td></tr>
  </table>

  <table cellpadding=0 cellspacing=0 border=0>
    <tr>
      <td><br></td>
    </tr>
    <tr>
      <td>
<pre class="sourceHeading">          Line data    Source code</pre>
<pre class="source">
<a name="1"><span class="lineNum">       1 </span>            : #include &quot;NeuralNetwork.h&quot;</a>
<a name="2"><span class="lineNum">       2 </span>            : #include &quot;HiddenLayer.h&quot;</a>
<a name="3"><span class="lineNum">       3 </span>            : #include &quot;InputLayer.h&quot;</a>
<a name="4"><span class="lineNum">       4 </span>            : #include &quot;Manager.h&quot;</a>
<a name="5"><span class="lineNum">       5 </span>            : #include &quot;OutputLayer.h&quot;</a>
<a name="6"><span class="lineNum">       6 </span>            : #include &quot;exception/NetworkException.h&quot;</a>
<a name="7"><span class="lineNum">       7 </span>            : </a>
<a name="8"><span class="lineNum">       8 </span>            : using namespace sipai;</a>
<a name="9"><span class="lineNum">       9 </span>            : </a>
<a name="10"><span class="lineNum">      10 </span><span class="lineCov">          4 : void NeuralNetwork::initialize() {</span></a>
<a name="11"><span class="lineNum">      11 </span><span class="lineCov">          4 :   auto inputLayer = new InputLayer();</span></a>
<a name="12"><span class="lineNum">      12 </span><span class="lineCov">          4 :   const auto &amp;network_params = Manager::getInstance().network_params;</span></a>
<a name="13"><span class="lineNum">      13 </span><span class="lineCov">          4 :   inputLayer-&gt;neurons.resize(network_params.input_size_x *</span></a>
<a name="14"><span class="lineNum">      14 </span><span class="lineCov">          4 :                              network_params.input_size_y);</span></a>
<a name="15"><span class="lineNum">      15 </span><span class="lineCov">          4 :   layers.push_back(inputLayer);</span></a>
<a name="16"><span class="lineNum">      16 </span>            : </a>
<a name="17"><span class="lineNum">      17 </span><span class="lineCov">          9 :   for (size_t i = 0; i &lt; network_params.hiddens_count; ++i) {</span></a>
<a name="18"><span class="lineNum">      18 </span><span class="lineCov">          5 :     auto hiddenLayer = new HiddenLayer();</span></a>
<a name="19"><span class="lineNum">      19 </span><span class="lineCov">          5 :     hiddenLayer-&gt;neurons.resize(network_params.hidden_size_x *</span></a>
<a name="20"><span class="lineNum">      20 </span><span class="lineCov">          5 :                                 network_params.hidden_size_y);</span></a>
<a name="21"><span class="lineNum">      21 </span><span class="lineCov">          5 :     SetActivationFunction(hiddenLayer,</span></a>
<a name="22"><span class="lineNum">      22 </span><span class="lineCov">          5 :                           network_params.hidden_activation_function,</span></a>
<a name="23"><span class="lineNum">      23 </span><span class="lineCov">          5 :                           network_params.hidden_activation_alpha);</span></a>
<a name="24"><span class="lineNum">      24 </span><span class="lineCov">          5 :     layers.push_back(hiddenLayer);</span></a>
<a name="25"><span class="lineNum">      25 </span>            :   }</a>
<a name="26"><span class="lineNum">      26 </span>            : </a>
<a name="27"><span class="lineNum">      27 </span><span class="lineCov">          4 :   auto outputLayer = new OutputLayer();</span></a>
<a name="28"><span class="lineNum">      28 </span><span class="lineCov">          4 :   outputLayer-&gt;neurons.resize(network_params.output_size_x *</span></a>
<a name="29"><span class="lineNum">      29 </span><span class="lineCov">          4 :                               network_params.output_size_y);</span></a>
<a name="30"><span class="lineNum">      30 </span><span class="lineCov">          4 :   SetActivationFunction(outputLayer, network_params.output_activation_function,</span></a>
<a name="31"><span class="lineNum">      31 </span><span class="lineCov">          4 :                         network_params.output_activation_alpha);</span></a>
<a name="32"><span class="lineNum">      32 </span><span class="lineCov">          4 :   layers.push_back(outputLayer);</span></a>
<a name="33"><span class="lineNum">      33 </span>            : </a>
<a name="34"><span class="lineNum">      34 </span><span class="lineCov">          4 :   bindLayers();</span></a>
<a name="35"><span class="lineNum">      35 </span><span class="lineCov">          4 :   initializeWeights();</span></a>
<a name="36"><span class="lineNum">      36 </span><span class="lineCov">          4 :   initializeNeighbors();</span></a>
<a name="37"><span class="lineNum">      37 </span><span class="lineCov">          4 : }</span></a>
<a name="38"><span class="lineNum">      38 </span>            : </a>
<a name="39"><span class="lineNum">      39 </span>            : std::vector&lt;RGBA&gt;</a>
<a name="40"><span class="lineNum">      40 </span><span class="lineCov">         20 : NeuralNetwork::forwardPropagation(const std::vector&lt;RGBA&gt; &amp;inputValues) {</span></a>
<a name="41"><span class="lineNum">      41 </span><span class="lineCov">         20 :   if (layers.front()-&gt;layerType != LayerType::InputLayer) {</span></a>
<a name="42"><span class="lineNum">      42 </span><span class="lineNoCov">          0 :     throw NetworkException(&quot;Invalid front layer type&quot;);</span></a>
<a name="43"><span class="lineNum">      43 </span>            :   }</a>
<a name="44"><span class="lineNum">      44 </span><span class="lineCov">         20 :   if (layers.back()-&gt;layerType != LayerType::OutputLayer) {</span></a>
<a name="45"><span class="lineNum">      45 </span><span class="lineNoCov">          0 :     throw NetworkException(&quot;Invalid back layer type&quot;);</span></a>
<a name="46"><span class="lineNum">      46 </span>            :   }</a>
<a name="47"><span class="lineNum">      47 </span><span class="lineCov">         20 :   ((InputLayer *)layers.front())-&gt;setInputValues(inputValues);</span></a>
<a name="48"><span class="lineNum">      48 </span><span class="lineCov">        200 :   for (auto &amp;layer : layers) {</span></a>
<a name="49"><span class="lineNum">      49 </span><span class="lineCov">        180 :     layer-&gt;forwardPropagation();</span></a>
<a name="50"><span class="lineNum">      50 </span>            :   }</a>
<a name="51"><span class="lineNum">      51 </span><span class="lineCov">         20 :   return ((OutputLayer *)layers.back())-&gt;getOutputValues();</span></a>
<a name="52"><span class="lineNum">      52 </span>            : }</a>
<a name="53"><span class="lineNum">      53 </span>            : </a>
<a name="54"><span class="lineNum">      54 </span><span class="lineCov">         16 : void NeuralNetwork::backwardPropagation(</span></a>
<a name="55"><span class="lineNum">      55 </span>            :     const std::vector&lt;RGBA&gt; &amp;expectedValues) {</a>
<a name="56"><span class="lineNum">      56 </span><span class="lineCov">         16 :   if (layers.back()-&gt;layerType != LayerType::OutputLayer) {</span></a>
<a name="57"><span class="lineNum">      57 </span><span class="lineNoCov">          0 :     throw NetworkException(&quot;Invalid back layer type&quot;);</span></a>
<a name="58"><span class="lineNum">      58 </span>            :   }</a>
<a name="59"><span class="lineNum">      59 </span><span class="lineCov">         16 :   ((OutputLayer *)layers.back())-&gt;computeErrors(expectedValues);</span></a>
<a name="60"><span class="lineNum">      60 </span><span class="lineCov">        160 :   for (auto it = layers.rbegin(); it != layers.rend(); ++it) {</span></a>
<a name="61"><span class="lineNum">      61 </span><span class="lineCov">        144 :     (*it)-&gt;backwardPropagation();</span></a>
<a name="62"><span class="lineNum">      62 </span>            :   }</a>
<a name="63"><span class="lineNum">      63 </span><span class="lineCov">         16 : }</span></a>
<a name="64"><span class="lineNum">      64 </span>            : </a>
<a name="65"><span class="lineNum">      65 </span><span class="lineCov">          5 : void NeuralNetwork::bindLayers() {</span></a>
<a name="66"><span class="lineNum">      66 </span><span class="lineCov">         30 :   for (size_t i = 0; i &lt; layers.size(); ++i) {</span></a>
<a name="67"><span class="lineNum">      67 </span><span class="lineCov">         25 :     if (i &gt; 0) {</span></a>
<a name="68"><span class="lineNum">      68 </span><span class="lineCov">         20 :       layers.at(i)-&gt;previousLayer = layers.at(i - 1);</span></a>
<a name="69"><span class="lineNum">      69 </span>            :     }</a>
<a name="70"><span class="lineNum">      70 </span><span class="lineCov">         25 :     if (i &lt; layers.size() - 1) {</span></a>
<a name="71"><span class="lineNum">      71 </span><span class="lineCov">         20 :       layers.at(i)-&gt;nextLayer = layers.at(i + 1);</span></a>
<a name="72"><span class="lineNum">      72 </span>            :     }</a>
<a name="73"><span class="lineNum">      73 </span>            :   }</a>
<a name="74"><span class="lineNum">      74 </span><span class="lineCov">          5 : }</span></a>
<a name="75"><span class="lineNum">      75 </span>            : </a>
<a name="76"><span class="lineNum">      76 </span><span class="lineCov">          4 : void NeuralNetwork::initializeWeights() const {</span></a>
<a name="77"><span class="lineNum">      77 </span><span class="lineCov">         26 :   for (auto layer : layers) {</span></a>
<a name="78"><span class="lineNum">      78 </span><span class="lineCov">         22 :     if (layer-&gt;previousLayer != nullptr) {</span></a>
<a name="79"><span class="lineNum">      79 </span><span class="lineCov">        141 :       for (auto &amp;n : layer-&gt;neurons) {</span></a>
<a name="80"><span class="lineNum">      80 </span><span class="lineCov">        123 :         n.initWeights(layer-&gt;previousLayer-&gt;neurons.size());</span></a>
<a name="81"><span class="lineNum">      81 </span>            :       }</a>
<a name="82"><span class="lineNum">      82 </span>            :     }</a>
<a name="83"><span class="lineNum">      83 </span>            :   }</a>
<a name="84"><span class="lineNum">      84 </span><span class="lineCov">          4 : }</span></a>
<a name="85"><span class="lineNum">      85 </span>            : </a>
<a name="86"><span class="lineNum">      86 </span><span class="lineCov">          4 : void NeuralNetwork::initializeNeighbors() {</span></a>
<a name="87"><span class="lineNum">      87 </span><span class="lineCov">          4 :   const auto &amp;network_params = Manager::getInstance().network_params;</span></a>
<a name="88"><span class="lineNum">      88 </span><span class="lineCov">         26 :   for (auto &amp;layer : layers) {</span></a>
<a name="89"><span class="lineNum">      89 </span>            :     int layer_size_x, layer_size_y;</a>
<a name="90"><span class="lineNum">      90 </span>            : </a>
<a name="91"><span class="lineNum">      91 </span><span class="lineCov">         22 :     if (dynamic_cast&lt;InputLayer *&gt;(layer)) {</span></a>
<a name="92"><span class="lineNum">      92 </span><span class="lineCov">          7 :       layer_size_x = network_params.input_size_x;</span></a>
<a name="93"><span class="lineNum">      93 </span><span class="lineCov">          7 :       layer_size_y = network_params.input_size_y;</span></a>
<a name="94"><span class="lineNum">      94 </span><span class="lineCov">         15 :     } else if (dynamic_cast&lt;HiddenLayer *&gt;(layer)) {</span></a>
<a name="95"><span class="lineNum">      95 </span><span class="lineCov">          8 :       layer_size_x = network_params.hidden_size_x;</span></a>
<a name="96"><span class="lineNum">      96 </span><span class="lineCov">          8 :       layer_size_y = network_params.hidden_size_y;</span></a>
<a name="97"><span class="lineNum">      97 </span><span class="lineCov">          7 :     } else if (dynamic_cast&lt;OutputLayer *&gt;(layer)) {</span></a>
<a name="98"><span class="lineNum">      98 </span><span class="lineCov">          7 :       layer_size_x = network_params.output_size_x;</span></a>
<a name="99"><span class="lineNum">      99 </span><span class="lineCov">          7 :       layer_size_y = network_params.output_size_y;</span></a>
<a name="100"><span class="lineNum">     100 </span>            :     }</a>
<a name="101"><span class="lineNum">     101 </span>            : </a>
<a name="102"><span class="lineNum">     102 </span><span class="lineCov">        161 :     for (size_t i = 0; i &lt; layer-&gt;neurons.size(); ++i) {</span></a>
<a name="103"><span class="lineNum">     103 </span><span class="lineCov">        139 :       addNeuronNeighbors(layer-&gt;neurons[i], layer, i, layer_size_x,</span></a>
<a name="104"><span class="lineNum">     104 </span>            :                          layer_size_y);</a>
<a name="105"><span class="lineNum">     105 </span>            :     }</a>
<a name="106"><span class="lineNum">     106 </span>            :   }</a>
<a name="107"><span class="lineNum">     107 </span><span class="lineCov">          4 : }</span></a>
<a name="108"><span class="lineNum">     108 </span>            : </a>
<a name="109"><span class="lineNum">     109 </span><span class="lineCov">        158 : void NeuralNetwork::addNeuronNeighbors(Neuron &amp;neuron, Layer *neuron_layer,</span></a>
<a name="110"><span class="lineNum">     110 </span>            :                                        size_t neuron_index, int layer_size_x,</a>
<a name="111"><span class="lineNum">     111 </span>            :                                        int layer_size_y,</a>
<a name="112"><span class="lineNum">     112 </span>            :                                        bool randomize_weight) {</a>
<a name="113"><span class="lineNum">     113 </span>            :   // Compute the coordinates of the neuron in the 2D grid</a>
<a name="114"><span class="lineNum">     114 </span><span class="lineCov">        158 :   int x = neuron_index % layer_size_x;</span></a>
<a name="115"><span class="lineNum">     115 </span><span class="lineCov">        158 :   int y = neuron_index / layer_size_x;</span></a>
<a name="116"><span class="lineNum">     116 </span>            : </a>
<a name="117"><span class="lineNum">     117 </span>            :   // For each possible direction (up, down, left, right), check if there</a>
<a name="118"><span class="lineNum">     118 </span>            :   // is a neuron in that direction and, if so, establish a connection</a>
<a name="119"><span class="lineNum">     119 </span>            :   std::vector&lt;std::pair&lt;int, int&gt;&gt; directions = {</a>
<a name="120"><span class="lineNum">     120 </span><span class="lineCov">        158 :       {-1, 0}, {1, 0}, {0, -1}, {0, 1}};</span></a>
<a name="121"><span class="lineNum">     121 </span><span class="lineCov">        790 :   for (auto [dx, dy] : directions) {</span></a>
<a name="122"><span class="lineNum">     122 </span><span class="lineCov">        632 :     int nx = x + dx;</span></a>
<a name="123"><span class="lineNum">     123 </span><span class="lineCov">        632 :     int ny = y + dy;</span></a>
<a name="124"><span class="lineNum">     124 </span><span class="lineCov">        632 :     if (nx &gt;= 0 &amp;&amp; nx &lt; layer_size_x &amp;&amp; ny &gt;= 0 &amp;&amp; ny &lt; layer_size_y) {</span></a>
<a name="125"><span class="lineNum">     125 </span><span class="lineCov">        382 :       int ni = ny * layer_size_x + nx;</span></a>
<a name="126"><span class="lineNum">     126 </span><span class="lineCov">        382 :       Neuron &amp;neighbor = neuron_layer-&gt;neurons[ni];</span></a>
<a name="127"><span class="lineNum">     127 </span>            : </a>
<a name="128"><span class="lineNum">     128 </span><span class="lineCov">        382 :       RGBA weight;</span></a>
<a name="129"><span class="lineNum">     129 </span><span class="lineCov">        382 :       if (randomize_weight) {</span></a>
<a name="130"><span class="lineNum">     130 </span>            :         // Create a connection with a random initial weight</a>
<a name="131"><span class="lineNum">     131 </span><span class="lineCov">        336 :         std::random_device rd;</span></a>
<a name="132"><span class="lineNum">     132 </span><span class="lineCov">        336 :         std::mt19937 gen(rd());</span></a>
<a name="133"><span class="lineNum">     133 </span><span class="lineCov">        336 :         std::uniform_real_distribution&lt;float&gt; dist(0.0f, 1.0f);</span></a>
<a name="134"><span class="lineNum">     134 </span><span class="lineCov">        336 :         weight = {dist(gen), dist(gen), dist(gen), dist(gen)};</span></a>
<a name="135"><span class="lineNum">     135 </span><span class="lineCov">        336 :       }</span></a>
<a name="136"><span class="lineNum">     136 </span>            : </a>
<a name="137"><span class="lineNum">     137 </span><span class="lineCov">        382 :       neuron.neighbors.push_back(Connection(&amp;neighbor, weight));</span></a>
<a name="138"><span class="lineNum">     138 </span>            :     }</a>
<a name="139"><span class="lineNum">     139 </span>            :   }</a>
<a name="140"><span class="lineNum">     140 </span><span class="lineCov">        158 : }</span></a>
<a name="141"><span class="lineNum">     141 </span>            : </a>
<a name="142"><span class="lineNum">     142 </span><span class="lineCov">         11 : void NeuralNetwork::SetActivationFunction(</span></a>
<a name="143"><span class="lineNum">     143 </span>            :     Layer *layer, EActivationFunction activation_function,</a>
<a name="144"><span class="lineNum">     144 </span>            :     float activation_alpha) const {</a>
<a name="145"><span class="lineNum">     145 </span><span class="lineCov">         11 :   switch (activation_function) {</span></a>
<a name="146"><span class="lineNum">     146 </span><span class="lineNoCov">          0 :   case EActivationFunction::ELU:</span></a>
<a name="147"><span class="lineNum">     147 </span><span class="lineNoCov">          0 :     layer-&gt;setActivationFunction(</span></a>
<a name="148"><span class="lineNum">     148 </span><span class="lineNoCov">          0 :         [activation_alpha](auto x) { return elu(x, activation_alpha); },</span></a>
<a name="149"><span class="lineNum">     149 </span><span class="lineNoCov">          0 :         [activation_alpha](auto x) {</span></a>
<a name="150"><span class="lineNum">     150 </span><span class="lineNoCov">          0 :           return eluDerivative(x, activation_alpha);</span></a>
<a name="151"><span class="lineNum">     151 </span>            :         });</a>
<a name="152"><span class="lineNum">     152 </span><span class="lineNoCov">          0 :     break;</span></a>
<a name="153"><span class="lineNum">     153 </span><span class="lineNoCov">          0 :   case EActivationFunction::LReLU:</span></a>
<a name="154"><span class="lineNum">     154 </span><span class="lineNoCov">          0 :     layer-&gt;setActivationFunction(leakyRelu, leakyReluDerivative);</span></a>
<a name="155"><span class="lineNum">     155 </span><span class="lineNoCov">          0 :     break;</span></a>
<a name="156"><span class="lineNum">     156 </span><span class="lineNoCov">          0 :   case EActivationFunction::PReLU:</span></a>
<a name="157"><span class="lineNum">     157 </span><span class="lineNoCov">          0 :     layer-&gt;setActivationFunction(</span></a>
<a name="158"><span class="lineNum">     158 </span><span class="lineNoCov">          0 :         [activation_alpha](auto x) {</span></a>
<a name="159"><span class="lineNum">     159 </span><span class="lineNoCov">          0 :           return parametricRelu(x, activation_alpha);</span></a>
<a name="160"><span class="lineNum">     160 </span>            :         },</a>
<a name="161"><span class="lineNum">     161 </span><span class="lineNoCov">          0 :         [activation_alpha](auto x) {</span></a>
<a name="162"><span class="lineNum">     162 </span><span class="lineNoCov">          0 :           return parametricReluDerivative(x, activation_alpha);</span></a>
<a name="163"><span class="lineNum">     163 </span>            :         });</a>
<a name="164"><span class="lineNum">     164 </span><span class="lineNoCov">          0 :     break;</span></a>
<a name="165"><span class="lineNum">     165 </span><span class="lineCov">         11 :   case EActivationFunction::ReLU:</span></a>
<a name="166"><span class="lineNum">     166 </span><span class="lineCov">         11 :     layer-&gt;setActivationFunction(relu, reluDerivative);</span></a>
<a name="167"><span class="lineNum">     167 </span><span class="lineCov">         11 :     break;</span></a>
<a name="168"><span class="lineNum">     168 </span><span class="lineNoCov">          0 :   case EActivationFunction::Sigmoid:</span></a>
<a name="169"><span class="lineNum">     169 </span><span class="lineNoCov">          0 :     layer-&gt;setActivationFunction(sigmoid, sigmoidDerivative);</span></a>
<a name="170"><span class="lineNum">     170 </span><span class="lineNoCov">          0 :     break;</span></a>
<a name="171"><span class="lineNum">     171 </span><span class="lineNoCov">          0 :   case EActivationFunction::Tanh:</span></a>
<a name="172"><span class="lineNum">     172 </span><span class="lineNoCov">          0 :     layer-&gt;setActivationFunction(tanhFunc, tanhDerivative);</span></a>
<a name="173"><span class="lineNum">     173 </span><span class="lineNoCov">          0 :     break;</span></a>
<a name="174"><span class="lineNum">     174 </span><span class="lineNoCov">          0 :   default:</span></a>
<a name="175"><span class="lineNum">     175 </span><span class="lineNoCov">          0 :     throw NetworkException(&quot;Unimplemented Activation Function&quot;);</span></a>
<a name="176"><span class="lineNum">     176 </span>            :   }</a>
<a name="177"><span class="lineNum">     177 </span><span class="lineCov">         11 : }</span></a>
<a name="178"><span class="lineNum">     178 </span>            : </a>
<a name="179"><span class="lineNum">     179 </span><span class="lineCov">         16 : void NeuralNetwork::updateWeights(float learning_rate) {</span></a>
<a name="180"><span class="lineNum">     180 </span><span class="lineCov">        160 :   for (auto &amp;layer : layers) {</span></a>
<a name="181"><span class="lineNum">     181 </span><span class="lineCov">        144 :     layer-&gt;updateWeights(learning_rate);</span></a>
<a name="182"><span class="lineNum">     182 </span>            :   }</a>
<a name="183"><span class="lineNum">     183 </span><span class="lineCov">         16 : }</span></a>
</pre>
      </td>
    </tr>
  </table>
  <br>

  <table width="100%" border=0 cellspacing=0 cellpadding=0>
    <tr><td class="ruler"><img src="../glass.png" width=3 height=3 alt=""></td></tr>
    <tr><td class="versionInfo">Generated by: <a href="http://ltp.sourceforge.net/coverage/lcov.php" target="_parent">LCOV version 1.14</a></td></tr>
  </table>
  <br>

</body>
</html>
