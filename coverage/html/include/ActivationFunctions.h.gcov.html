<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>LCOV - lcov.info - include/ActivationFunctions.h</title>
  <link rel="stylesheet" type="text/css" href="../gcov.css">
</head>

<body>

  <table width="100%" border=0 cellspacing=0 cellpadding=0>
    <tr><td class="title">LCOV - code coverage report</td></tr>
    <tr><td class="ruler"><img src="../glass.png" width=3 height=3 alt=""></td></tr>

    <tr>
      <td width="100%">
        <table cellpadding=1 border=0 width="100%">
          <tr>
            <td width="10%" class="headerItem">Current view:</td>
            <td width="35%" class="headerValue"><a href="../index.html">top level</a> - <a href="index.html">include</a> - ActivationFunctions.h<span style="font-size: 80%;"> (source / <a href="ActivationFunctions.h.func-sort-c.html">functions</a>)</span></td>
            <td width="5%"></td>
            <td width="15%"></td>
            <td width="10%" class="headerCovTableHead">Hit</td>
            <td width="10%" class="headerCovTableHead">Total</td>
            <td width="15%" class="headerCovTableHead">Coverage</td>
          </tr>
          <tr>
            <td class="headerItem">Test:</td>
            <td class="headerValue">lcov.info</td>
            <td></td>
            <td class="headerItem">Lines:</td>
            <td class="headerCovTableEntry">20</td>
            <td class="headerCovTableEntry">20</td>
            <td class="headerCovTableEntryHi">100.0 %</td>
          </tr>
          <tr>
            <td class="headerItem">Date:</td>
            <td class="headerValue">2024-03-20 23:05:26</td>
            <td></td>
            <td class="headerItem">Functions:</td>
            <td class="headerCovTableEntry">12</td>
            <td class="headerCovTableEntry">12</td>
            <td class="headerCovTableEntryHi">100.0 %</td>
          </tr>
          <tr><td><img src="../glass.png" width=3 height=3 alt=""></td></tr>
        </table>
      </td>
    </tr>

    <tr><td class="ruler"><img src="../glass.png" width=3 height=3 alt=""></td></tr>
  </table>

  <table cellpadding=0 cellspacing=0 border=0>
    <tr>
      <td><br></td>
    </tr>
    <tr>
      <td>
<pre class="sourceHeading">          Line data    Source code</pre>
<pre class="source">
<a name="1"><span class="lineNum">       1 </span>            : /**</a>
<a name="2"><span class="lineNum">       2 </span>            :  * @file ActivationFunctions.h</a>
<a name="3"><span class="lineNum">       3 </span>            :  * @author Damien Balima (www.dams-labs.net)</a>
<a name="4"><span class="lineNum">       4 </span>            :  * @brief Activation functions</a>
<a name="5"><span class="lineNum">       5 </span>            :  * @date 2024-03-07</a>
<a name="6"><span class="lineNum">       6 </span>            :  *</a>
<a name="7"><span class="lineNum">       7 </span>            :  * @copyright Damien Balima (c) CC-BY-NC-SA-4.0 2024</a>
<a name="8"><span class="lineNum">       8 </span>            :  *</a>
<a name="9"><span class="lineNum">       9 </span>            :  */</a>
<a name="10"><span class="lineNum">      10 </span>            : </a>
<a name="11"><span class="lineNum">      11 </span>            : #pragma once</a>
<a name="12"><span class="lineNum">      12 </span>            : #include &lt;map&gt;</a>
<a name="13"><span class="lineNum">      13 </span>            : #include &lt;math.h&gt;</a>
<a name="14"><span class="lineNum">      14 </span>            : #include &lt;string&gt;</a>
<a name="15"><span class="lineNum">      15 </span>            : </a>
<a name="16"><span class="lineNum">      16 </span>            : namespace sipai {</a>
<a name="17"><span class="lineNum">      17 </span>            : /**</a>
<a name="18"><span class="lineNum">      18 </span>            :  * @brief Activation Function enum.</a>
<a name="19"><span class="lineNum">      19 </span>            :  *</a>
<a name="20"><span class="lineNum">      20 </span>            :  */</a>
<a name="21"><span class="lineNum">      21 </span>            : enum class EActivationFunction { ELU, LReLU, PReLU, ReLU, Sigmoid, Tanh };</a>
<a name="22"><span class="lineNum">      22 </span>            : </a>
<a name="23"><span class="lineNum">      23 </span>            : const std::map&lt;std::string, EActivationFunction, std::less&lt;&gt;&gt; activation_map{</a>
<a name="24"><span class="lineNum">      24 </span>            :     {&quot;ELU&quot;, EActivationFunction::ELU},</a>
<a name="25"><span class="lineNum">      25 </span>            :     {&quot;LReLU&quot;, EActivationFunction::LReLU},</a>
<a name="26"><span class="lineNum">      26 </span>            :     {&quot;PReLU&quot;, EActivationFunction::PReLU},</a>
<a name="27"><span class="lineNum">      27 </span>            :     {&quot;ReLU&quot;, EActivationFunction::ReLU},</a>
<a name="28"><span class="lineNum">      28 </span>            :     {&quot;Sigmoid&quot;, EActivationFunction::Sigmoid},</a>
<a name="29"><span class="lineNum">      29 </span>            :     {&quot;Tanh&quot;, EActivationFunction::Tanh}};</a>
<a name="30"><span class="lineNum">      30 </span>            : </a>
<a name="31"><span class="lineNum">      31 </span>            : /**</a>
<a name="32"><span class="lineNum">      32 </span>            :  * @brief the sigmoid function is commonly used as the</a>
<a name="33"><span class="lineNum">      33 </span>            :  * activation function during the forward propagation step. The reason for this</a>
<a name="34"><span class="lineNum">      34 </span>            :  * is that the sigmoid function maps any input value into a range between 0 and</a>
<a name="35"><span class="lineNum">      35 </span>            :  * 1, which can be useful for outputting probabilities, among other things.</a>
<a name="36"><span class="lineNum">      36 </span>            :  * The sigmoid derivative can be expressed in terms of the output of</a>
<a name="37"><span class="lineNum">      37 </span>            :  * the sigmoid function itself: if σ(x) is the sigmoid function, then its</a>
<a name="38"><span class="lineNum">      38 </span>            :  * derivative σ'(x) can be computed as σ(x) * (1 - σ(x)).</a>
<a name="39"><span class="lineNum">      39 </span>            :  */</a>
<a name="40"><span class="lineNum">      40 </span><span class="lineCov">          8 : inline auto sigmoid = [](auto x) { return 1.0f / (1.0f + exp(-x)); };</span></a>
<a name="41"><span class="lineNum">      41 </span><span class="lineCov">          4 : inline auto sigmoidDerivative = [](auto x) {</span></a>
<a name="42"><span class="lineNum">      42 </span><span class="lineCov">          4 :   float sigmoidValue = sigmoid(x);</span></a>
<a name="43"><span class="lineNum">      43 </span><span class="lineCov">          4 :   return sigmoidValue * (1 - sigmoidValue);</span></a>
<a name="44"><span class="lineNum">      44 </span>            : };</a>
<a name="45"><span class="lineNum">      45 </span>            : </a>
<a name="46"><span class="lineNum">      46 </span>            : /**</a>
<a name="47"><span class="lineNum">      47 </span>            :  * @brief Tanh Function (Hyperbolic Tangent): This function is similar to the</a>
<a name="48"><span class="lineNum">      48 </span>            :  * sigmoid function but maps the input to a range between -1 and 1. It is often</a>
<a name="49"><span class="lineNum">      49 </span>            :  * used in the hidden layers of a neural network.</a>
<a name="50"><span class="lineNum">      50 </span>            :  */</a>
<a name="51"><span class="lineNum">      51 </span><span class="lineCov">          6 : inline auto tanhFunc = [](auto x) { return tanh(x); };</span></a>
<a name="52"><span class="lineNum">      52 </span><span class="lineCov">          6 : inline auto tanhDerivative = [](auto x) {</span></a>
<a name="53"><span class="lineNum">      53 </span><span class="lineCov">          6 :   float tanhValue = tanh(x);</span></a>
<a name="54"><span class="lineNum">      54 </span><span class="lineCov">          6 :   return 1 - tanhValue * tanhValue;</span></a>
<a name="55"><span class="lineNum">      55 </span>            : };</a>
<a name="56"><span class="lineNum">      56 </span>            : </a>
<a name="57"><span class="lineNum">      57 </span>            : /**</a>
<a name="58"><span class="lineNum">      58 </span>            :  * @brief ReLU Function (Rectified Linear Unit): This function outputs the input</a>
<a name="59"><span class="lineNum">      59 </span>            :  * directly if it’s positive; otherwise, it outputs zero. It has become very</a>
<a name="60"><span class="lineNum">      60 </span>            :  * popular in recent years because it helps to alleviate the vanishing gradient</a>
<a name="61"><span class="lineNum">      61 </span>            :  * problem.</a>
<a name="62"><span class="lineNum">      62 </span>            :  * @param Unit</a>
<a name="63"><span class="lineNum">      63 </span>            :  * @return ReLU</a>
<a name="64"><span class="lineNum">      64 </span>            :  */</a>
<a name="65"><span class="lineNum">      65 </span><span class="lineCov">       3604 : inline auto relu = [](auto x) { return std::max(0.0f, x); };</span></a>
<a name="66"><span class="lineNum">      66 </span><span class="lineCov">       1156 : inline auto reluDerivative = [](auto x) { return x &gt; 0 ? 1.0f : 0.0f; };</span></a>
<a name="67"><span class="lineNum">      67 </span>            : </a>
<a name="68"><span class="lineNum">      68 </span>            : /**</a>
<a name="69"><span class="lineNum">      69 </span>            :  * @brief Leaky ReLU: This is a variant of ReLU that allows small negative</a>
<a name="70"><span class="lineNum">      70 </span>            :  * values when the input is less than zero. It can help to alleviate the dying</a>
<a name="71"><span class="lineNum">      71 </span>            :  * ReLU problem where neurons become inactive and only output zero.</a>
<a name="72"><span class="lineNum">      72 </span>            :  */</a>
<a name="73"><span class="lineNum">      73 </span><span class="lineCov">          4 : inline auto leakyRelu = [](auto x) { return std::max(0.01f * x, x); };</span></a>
<a name="74"><span class="lineNum">      74 </span><span class="lineCov">          4 : inline auto leakyReluDerivative = [](auto x) { return x &gt; 0 ? 1.0f : 0.01f; };</span></a>
<a name="75"><span class="lineNum">      75 </span>            : </a>
<a name="76"><span class="lineNum">      76 </span>            : /**</a>
<a name="77"><span class="lineNum">      77 </span>            :  * @brief  Parametric ReLU (PReLU) is a type of leaky ReLU that, instead of</a>
<a name="78"><span class="lineNum">      78 </span>            :  * having a predetermined slope like 0.01, learns the slope during training.</a>
<a name="79"><span class="lineNum">      79 </span>            :  * This can give it a bit more flexibility and help it to learn more complex</a>
<a name="80"><span class="lineNum">      80 </span>            :  * patterns</a>
<a name="81"><span class="lineNum">      81 </span>            :  */</a>
<a name="82"><span class="lineNum">      82 </span><span class="lineCov">          4 : inline auto parametricRelu = [](auto x, auto alpha) {</span></a>
<a name="83"><span class="lineNum">      83 </span><span class="lineCov">          4 :   return std::max(alpha * x, x);</span></a>
<a name="84"><span class="lineNum">      84 </span>            : };</a>
<a name="85"><span class="lineNum">      85 </span><span class="lineCov">          4 : inline auto parametricReluDerivative = [](auto x, auto alpha) {</span></a>
<a name="86"><span class="lineNum">      86 </span><span class="lineCov">          4 :   return x &gt; 0 ? 1.0f : alpha;</span></a>
<a name="87"><span class="lineNum">      87 </span>            : };</a>
<a name="88"><span class="lineNum">      88 </span>            : </a>
<a name="89"><span class="lineNum">      89 </span>            : /**</a>
<a name="90"><span class="lineNum">      90 </span>            :  * @brief  the Exponential Linear Units (ELUs) are a great choice as they</a>
<a name="91"><span class="lineNum">      91 </span>            :  * take on negative values when the input is less than zero, which allows them</a>
<a name="92"><span class="lineNum">      92 </span>            :  * to push mean unit activations closer to zero like batch normalization. Unlike</a>
<a name="93"><span class="lineNum">      93 </span>            :  * ReLUs, ELUs have a nonzero gradient for negative input, which avoids the</a>
<a name="94"><span class="lineNum">      94 </span>            :  * “dead neuron” problem.</a>
<a name="95"><span class="lineNum">      95 </span>            :  *</a>
<a name="96"><span class="lineNum">      96 </span>            :  */</a>
<a name="97"><span class="lineNum">      97 </span><span class="lineCov">          6 : inline auto elu = [](auto x, auto alpha) {</span></a>
<a name="98"><span class="lineNum">      98 </span><span class="lineCov">          6 :   return x &gt;= 0 ? x : alpha * (exp(x) - 1);</span></a>
<a name="99"><span class="lineNum">      99 </span>            : };</a>
<a name="100"><span class="lineNum">     100 </span><span class="lineCov">          4 : inline auto eluDerivative = [](auto x, auto alpha) {</span></a>
<a name="101"><span class="lineNum">     101 </span><span class="lineCov">          4 :   return x &gt;= 0 ? 1 : alpha * exp(x);</span></a>
<a name="102"><span class="lineNum">     102 </span>            : };</a>
<a name="103"><span class="lineNum">     103 </span>            : } // namespace sipai</a>
</pre>
      </td>
    </tr>
  </table>
  <br>

  <table width="100%" border=0 cellspacing=0 cellpadding=0>
    <tr><td class="ruler"><img src="../glass.png" width=3 height=3 alt=""></td></tr>
    <tr><td class="versionInfo">Generated by: <a href="http://ltp.sourceforge.net/coverage/lcov.php" target="_parent">LCOV version 1.14</a></td></tr>
  </table>
  <br>

</body>
</html>
