#version 450

// enable debug printf extension only in debug mode
// for debug printf infos, see https://www.lunarg.com/wp-content/uploads/2021/08/Using-Debug-Printf-02August2021.pdf
//#extension GL_EXT_debug_printf : enable

/**
 * *TEMPLATE* GLSL training compute shader for Vulkan, by Dams
 * Damien Balima (c) CC-BY-NC-SA-4.0 2024
 * Do not edit
 */

const int OUTPUT_SIZE_X = %%OUTPUT_SIZE_X%%;
const int OUTPUT_SIZE_Y = %%OUTPUT_SIZE_Y%%;
const int HIDDEN_SIZE_X = %%HIDDEN_SIZE_X%%;
const int HIDDEN_SIZE_Y = %%HIDDEN_SIZE_Y%%;
const int INPUT_SIZE_X = %%INPUT_SIZE_X%%;
const int INPUT_SIZE_Y = %%INPUT_SIZE_Y%%;

// Enum mapping, do not change
const int ELU = 0;
const int LReLU = 1;
const int PReLU = 2;
const int ReLU = 3;
const int Sigmoid = 4;
const int Tanh = 5;

struct Neighbor {
  uint is_used;
  uint index_x;
  uint index_y;
  vec4 weight; // weight of the neighbor connection
};

struct HiddenNeuron {
  uint index_x;
  uint index_y;
  vec4 weights[INPUT_SIZE_Y][INPUT_SIZE_X];
  Neighbor neighbors[4];
};

struct OutputNeuron {
  uint index_x;
  uint index_y;
  vec4 weights[HIDDEN_SIZE_Y][HIDDEN_SIZE_X];
  Neighbor neighbors[4];
};

// Parameters binding
layout(std430, binding = 0) buffer readonly Parameters {
  float learning_rate;
  float error_min;
  float error_max;
}
params;

// Layers binding
layout(std430, binding = 1) buffer readonly InputLayer {
  float activation_alpha;
  uint activation_function;
  uint size_x;
  uint size_y;
}
inputLayerBuffer;

layout(std430, binding = 2) buffer OutputLayer {
  OutputNeuron neurons[OUTPUT_SIZE_Y][OUTPUT_SIZE_X];
  vec4 errors[OUTPUT_SIZE_Y][OUTPUT_SIZE_X];
  float activation_alpha;
  uint activation_function;
  uint size_x;
  uint size_y;
}
outputLayerBuffer;

layout(std430, binding = 3) buffer HiddenLayer1 {
  HiddenNeuron neurons[HIDDEN_SIZE_Y][HIDDEN_SIZE_X];
  vec4 values[HIDDEN_SIZE_Y][HIDDEN_SIZE_X];
  vec4 errors[HIDDEN_SIZE_Y][HIDDEN_SIZE_X];
  float activation_alpha;
  uint activation_function;
  uint size_x;
  uint size_y;
}
hiddenLayer1Buffer;

// Data binding
layout(std430, binding = 4) buffer readonly InputData {
  vec4 inputValues[INPUT_SIZE_Y][INPUT_SIZE_X];  
  vec4 targetValues[OUTPUT_SIZE_Y][OUTPUT_SIZE_X];  
  bool is_validation;
}
inputDataBuffer;

layout(std430, binding = 5) buffer writeonly OutputData { 
  vec4 outputValues[OUTPUT_SIZE_Y][OUTPUT_SIZE_X];
}
outputDataBuffer;

layout(std430, binding = 6) buffer writeonly OutputLoss { 
  float loss;
}
lossBuffer;

layout(std430, binding = 7) buffer SharedOutputValues { 
  vec4 values[OUTPUT_SIZE_Y][OUTPUT_SIZE_X];
}
sharedOutputValues;

layout(std430, binding = 8) buffer SharedOutputLoss { 
  float values[OUTPUT_SIZE_Y][OUTPUT_SIZE_X];
}
sharedOutputLoss;

// Functions
vec4 derivativeFunction(vec4 value, uint activation_function,
                        float activation_alpha) {
  bvec4 mask;
  switch (activation_function) {
  case ELU:
    mask = greaterThanEqual(value, vec4(0.0));
    value = mix(vec4(1.0), activation_alpha * exp(value), mask);
    break;
  case LReLU:
    value = clamp(value, 0.01, 1.0);
    break;
  case PReLU:
    mask = greaterThanEqual(value, vec4(0.0));
    value = mix(vec4(1.0), vec4(activation_alpha), mask);
    break;
  case ReLU:
    mask = greaterThanEqual(value, vec4(0.0));
    value = mix(vec4(1.0), vec4(0.0), mask);
    break;
  case Sigmoid:
    value *= (1 - value);
    break;
  case Tanh: // shifted tanh [-1,1] to [0,1]
    value = (tanh(value) / 2.0) + 0.5;
    value = 1 - value * value;
    break;
  default:
    break;
  }
  return value;
}

void backwardHiddenLayer1(uint index_x, uint index_y) {
  if (index_y >= HIDDEN_SIZE_Y || index_x >= HIDDEN_SIZE_X) {
    return;
  }
  vec4 error = vec4(0.0);

  // Add next layer neurons error ponderated with weights for this neuron
  for (uint out_y = 0; out_y < OUTPUT_SIZE_Y; out_y++) {
    for (uint out_x = 0; out_x < OUTPUT_SIZE_X; out_x++) {
      vec4 out_error = outputLayerBuffer.errors[out_y][out_x];
      vec4 out_weight =
          outputLayerBuffer.neurons[out_y][out_x].weights[index_y][index_x];
      error += (out_error * out_weight);
    }
  }

  // Consider errors of adjacent neurons
  for (uint i = 0;
       i < hiddenLayer1Buffer.neurons[index_y][index_x].neighbors.length(); i++) {
    if (hiddenLayer1Buffer.neurons[index_y][index_x].neighbors[i].is_used == 0) {
      continue;
    }
    error += hiddenLayer1Buffer.neurons[index_y][index_x].neighbors[i].weight *
             hiddenLayer1Buffer.errors
                 [hiddenLayer1Buffer.neurons[index_y][index_x].neighbors[i].index_y]
                 [hiddenLayer1Buffer.neurons[index_y][index_x].neighbors[i].index_x];
  }
  //debugPrintfEXT("[DEBUG][BACKWARDHIDDENLAYER1] error [%i][%i] = %v4f", index_y, index_x, error);

  // Use the derivative of the activation function
  vec4 derivatedError = derivativeFunction(
      error, hiddenLayer1Buffer.activation_function, hiddenLayer1Buffer.activation_alpha);
  hiddenLayer1Buffer.errors[index_y][index_x] =
      clamp(derivatedError, params.error_min, params.error_max);
  //debugPrintfEXT("[DEBUG][BACKWARDHIDDENLAYER1] hiddenLayer1Buffer.errors[%i][%i] = %v4f (not clamped), %v4f (clamped)", index_y, index_x, derivatedError, hiddenLayer1Buffer.errors[index_y][index_x]);
}

void main() {
  // Get the index of the neuron (one global thread per neuron)
  uint index_x = gl_GlobalInvocationID.x;
  uint index_y = gl_GlobalInvocationID.y;

  // Backward Propagation part 2 (not for validation)
  if (inputDataBuffer.is_validation) {
    return;
  }  
  backwardHiddenLayer1(index_x, index_y); 

   // Then global threads synchro before next step
}